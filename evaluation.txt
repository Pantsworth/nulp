*-----------------*---------*--------------*
|     Problem     |  Bayes  |  Best Bayes  |
*-----------------*---------*--------------*
|    Precision    |  0.739  |    0.803     |
*-----------------*---------*--------------*
|     Recall      |  0.601  |    0.998     |
*-----------------*---------*--------------*
|    f-measure    |  0.663  |    0.890     |
*-----------------*---------*--------------*

The best bayes looked at bigram presence and stem presence whereas the regular bayes looked exclusively at unigram
presence. This is much more effective because it accounts for affixed versions of words as well as combinations of words
whose meanings depend on each other. For example, "not" and "good" might appear in primarily positive reviews but
"not good" might appear in exclusively negative ones. This best model accounts for this.

To extend this model we could look take into account frequency as well as presence. We could also consider different
weightings of variables (presence and frequency of stems and bigrams) when calculating the positive and negative
probabilities. We could also have considered stems and bigrams simultaneously by looking at consecutive pairs of stems.



APPENDIX A:

Here's the output of our evaluate.py script if you had wanted a more detailed table:

Running subsection 1 of 10
  training...
  classifying...
   0.746170678337 0.607843137255 0.669941060904
  training...
  classifying...
   0.810869565217 0.997326203209 0.89448441247
Running subsection 2 of 10
  training...
  classifying...
   0.726447219069 0.571428571429 0.63968015992
  training...
  classifying...
   0.807359307359 0.999107142857 0.893056664006
Running subsection 3 of 10
  training...
  classifying...
   0.751152073733 0.580071174377 0.654618473896
  training...
  classifying...
   0.811866859624 0.998220640569 0.895450917797
Running subsection 4 of 10
  training...
  classifying...
   0.757142857143 0.611357586513 0.676485027
  training...
  classifying...
   0.812274368231 0.998225377107 0.895700636943
Running subsection 5 of 10
  training...
  classifying...
   0.741228070175 0.611211573237 0.669970267592
  training...
  classifying...
   0.797687861272 0.998191681736 0.886746987952
Running subsection 6 of 10
  training...
  classifying...
   0.737704918033 0.607560756076 0.666337611056
  training...
  classifying...
   0.801449275362 0.995499549955 0.887996788438
Running subsection 7 of 10
  training...
  classifying...
   0.731991525424 0.625339366516 0.674475353831
  training...
  classifying...
   0.797111913357 0.999095022624 0.886746987952
Running subsection 8 of 10
  training...
  classifying...
   0.719088937093 0.610497237569 0.660358565737
  training...
  classifying...
   0.783646888567 0.997237569061 0.877633711507
Running subsection 9 of 10
  training...
  classifying...
   0.737373737374 0.590296495957 0.655688622754
  training...
  classifying...
   0.803183791606 0.99730458221 0.889779559118
Running subsection 10 of 10
  training...
  classifying...
   0.740740740741 0.591928251121 0.658025922233
  training...
  classifying...
   0.804049168474 0.99730941704 0.8903122498
Averages:
   0.738904075712 0.600753415005 0.662558106492
   0.802949899907 0.997751718637 0.889790891598